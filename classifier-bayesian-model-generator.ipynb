{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from predictionhelper.ipynb\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import loadnotebook\n",
    "from predictionhelper import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113700\n"
     ]
    }
   ],
   "source": [
    "sets = [x for x in range(1, 34)]\n",
    "demo_config = {6 : sets}\n",
    "\n",
    "df_data = get_data(config=demo_config, pure=True, refresh=False).reset_index(drop=True)\n",
    "print(len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pci_data = df_data[df_data[\"PCI\"].isin(whitelist_PCI)]\n",
    "beam_columns = [c for c in df_data if \"beam\" in c]\n",
    "pci_data = pci_data.drop([\"RSRP\", \"RSRQ\", \"SNR\"]+beam_columns, axis=1)\n",
    "pci_data = pci_data.drop_duplicates()\n",
    "pci_data = pci_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6356 2745\n"
     ]
    }
   ],
   "source": [
    "pci_train, pci_test = pd.DataFrame(), pd.DataFrame()\n",
    "pci_train_dict, pci_test_dict = {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = train_test_split(pci_data[pci_data.set==s], test_size=0.3, random_state=32)\n",
    "        pci_train = pci_train.append(a)\n",
    "        pci_test = pci_test.append(b)  \n",
    "        pci_train_dict[(p, s)] = a\n",
    "        pci_test_dict[(p, s)] = b\n",
    "print(len(pci_train), len(pci_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pci_train = pci_train.drop([\"PCI\"], axis=1)\n",
    "y_pci_train = np.array(pci_train.PCI.apply(lambda x : pci_encode[x]).values.tolist())\n",
    "x_pci_test = pci_test.drop([\"PCI\"], axis=1)\n",
    "y_pci_test = np.array(pci_test.PCI.apply(lambda x : pci_encode[x]).values.tolist())\n",
    "\n",
    "x_pci_train_dict, y_pci_train_dict, x_pci_test_dict, y_pci_test_dict = {}, {}, {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = pci_train_dict[(p,s)], pci_test_dict[(p,s)]\n",
    "        x_pci_train_dict[(p, s)] = a.drop([\"PCI\"], axis=1)\n",
    "        y_pci_train_dict[(p, s)] = np.array(a.PCI.apply(lambda x : pci_encode[x]).values.tolist())\n",
    "        x_pci_test_dict[(p, s)] = b.drop([\"PCI\"], axis=1)\n",
    "        y_pci_test_dict[(p, s)] = np.array(b.PCI.apply(lambda x : pci_encode[x]).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class svm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {}\n",
    "        params['C'] = param['C'] if param['C'] > 0 else 0.1\n",
    "        params['random_state'] = int(param['random_state'])\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, C=1.0, random_state=0):\n",
    "        params = {'C':C, 'random_state':random_state}\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model = svm.LinearSVC(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'random_state':0}\n",
    "# model = svm.LinearSVC(**svm_params)\n",
    "# model.fit(x_pci_train, y_pci_train)\n",
    "\n",
    "# y_pci_pred = model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.weights = ['uniform', 'distance']\n",
    "        self.algorithms = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'n_neighbors':7}\n",
    "        params['weights'] = self.weights[int(param['weight'])]\n",
    "        params['algorithm'] = self.algorithms[int(param['algorithm'])]\n",
    "        params['leaf_size'] = int(param['leaf_size'])\n",
    "        params['p'] = int(param['p'])\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, weight, algorithm, leaf_size=100, p=2):\n",
    "        params = {}\n",
    "        params['weight'] = weight\n",
    "        params['algorithm'] = algorithm\n",
    "        params['leaf_size'] = int(leaf_size)\n",
    "        params['p'] = int(p)\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model = KNeighborsClassifier(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pci_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pci_pred]\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kt = knn_target(x_pci_train[location_col], y_pci_train, \n",
    "#                 x_pci_test[location_col], y_pci_test)\n",
    "# kBO = BayesianOptimization(kt.evaluate, {'weight': (0, 1),\n",
    "#                                         'algorithm' : (0, 3),\n",
    "#                                         'leaf_size' : (5, 50),\n",
    "#                                         'p': (1, 2),},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# kBO.maximize(init_points=20, n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = kt.clean_param(kBO.res['max']['max_params'])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors': 6,\n",
    " 'weights': 'uniform',\n",
    " 'algorithm': 'kd_tree',\n",
    " 'leaf_size': 49,\n",
    " 'p': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNeighborsClassifier(**knn_params)\n",
    "# model.fit(x_pci_train, y_pci_train)\n",
    "\n",
    "# y_pci_pred = model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xgboost_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        booster_dict = {1:'gbtree', 2:'gblinear', 3:'dart'}\n",
    "        params = {'base_score':0.5, 'booster':'gbtree', 'missing':None, 'n_estimators':100, \n",
    "                  'n_jobs':1, 'objective':'multi:softmax', 'random_state':1, \n",
    "                  'reg_lambda':1, 'alpha':0, 'scale_pos_weight':1, \n",
    "                  'subsample':1, 'colsample_bytree':1, 'colsample_bylevel':1}\n",
    "\n",
    "        params['learning_rate'] = param['learning_rate']/100\n",
    "        params['booster'] = booster_dict[int(param['booster'])]\n",
    "        params['gamma'] = param['gamma']\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_delta_weight'] = int(param['max_delta_weight'])\n",
    "        params['rate_drop'] = param['rate_drop']\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, learning_rate, booster, gamma, max_depth,  \n",
    "                     min_child_weight, max_delta_weight, rate_drop):\n",
    "\n",
    "        params = {}\n",
    "        params['learning_rate'] = learning_rate\n",
    "        params['booster'] = booster\n",
    "        params['gamma'] = gamma\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['max_delta_weight'] = max_delta_weight\n",
    "        params['rate_drop'] = rate_drop\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        xgb_model = XGBClassifier(**params)\n",
    "        xgb_model.fit(self.x_train, self.y_train)\n",
    "        y_pci_pred = xgb_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pci_pred]\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xt = xgboost_target(x_pci_train, y_pci_train, x_pci_test, y_pci_test)\n",
    "# xgbBO = BayesianOptimization(xt.evaluate, {'learning_rate': (1, 12),\n",
    "#                                             'booster' : (1, 3),\n",
    "#                                             'gamma' : (0, 50),\n",
    "#                                             'max_depth': (3, 12),\n",
    "#                                             'min_child_weight': (1, 1),\n",
    "#                                             'max_delta_weight': (1, 20),\n",
    "#                                             'rate_drop': (0, 1)},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# xgbBO.maximize(init_points=10, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = xt.clean_param(xgbBO.res['max']['max_params'])\n",
    "# xgb_model = XGBClassifier(**params)\n",
    "# xgb_model.fit(x_pci_train, y_pci_train)\n",
    "\n",
    "# y_pci_pred = xgb_model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {'base_score': 0.5,\n",
    " 'booster': 'gbtree',\n",
    " 'missing': None,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 1,\n",
    " 'objective': 'multi:softmax',\n",
    " 'random_state': 1,\n",
    " 'reg_lambda': 1,\n",
    " 'alpha': 0,\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 1,\n",
    " 'learning_rate': 0.0536444221653737,\n",
    " 'gamma': 8.491520978228445,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 1,\n",
    " 'max_delta_weight': 12,\n",
    " 'rate_drop': 0.9445947559908133}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = XGBClassifier(**xgboost_params)\n",
    "# xgb_model.fit(x_pci_train, y_pci_train)\n",
    "\n",
    "# y_pci_pred = xgb_model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lgbm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':1.0, \n",
    "                  'importance_type':'split', \n",
    "                  'min_child_samples':20, 'min_split_gain':0.0, 'n_estimators':100, 'objective':None,\n",
    "                  'random_state':0, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "                  'subsample':1.0, 'subsample_for_bin':200000, 'subsample_freq':0}\n",
    "        params['num_leaves'] = int(param['num_leaves'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['learning_rate'] = param['learning_rate'] / 100\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, min_child_weight, learning_rate, max_depth, num_leaves):\n",
    "        params = {'num_leaves':num_leaves, \n",
    "                  'min_child_weight':min_child_weight, \n",
    "                  'max_depth':max_depth, \n",
    "                  'learning_rate':learning_rate}\n",
    "        \n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        lgbm_model = LGBMClassifier(**params )\n",
    "        lgbm_model.fit(self.x_train, self.y_train)\n",
    "        y_pci_pred = lgbm_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pci_pred]\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lt = lgbm_target(x_pci_train, y_pci_train, x_pci_test, y_pci_test)\n",
    "# lgbmBO = BayesianOptimization(lt.evaluate, {'min_child_weight': (0.01, 1),\n",
    "#                                               'learning_rate': (1, 10),\n",
    "#                                               'max_depth': (-1, 15),\n",
    "#                                               'num_leaves': (5, 50)}, \n",
    "#                              random_state=3)\n",
    "\n",
    "# lgbmBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = lt.clean_param(lgbmBO.res['max']['max_params'])\n",
    "# lgbm_model = LGBMClassifier(**params )\n",
    "# lgbm_model.fit(x_pci_train, y_pci_train)\n",
    "# y_pci_pred = lgbm_model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 36,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': 2,\n",
    " 'learning_rate': 0.09783958802256404}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'learning_rate' : 0.099387, 'max_depth' : 14, 'min_child_weight':0, 'num_leaves':5}\n",
    "# lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "# lgbm_model.fit(x_pci_train, y_pci_train)\n",
    "# y_pci_pred = lgbm_model.predict(x_pci_test)\n",
    "# predictions = [round(value) for value in y_pci_pred]\n",
    "# accuracy = accuracy_score(y_pci_test, predictions)\n",
    "# print(1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_matrix = np.empty((4, 3, 34))\n",
    "nrmse_matrix[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test) :\n",
    "    if 'xgb' in model_name : \n",
    "        return xgboost_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    elif 'lgbm' in model_name : \n",
    "        return lgbm_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    elif 'knn' in model_name : \n",
    "        return knn_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    elif 'svm' in model_name : \n",
    "        return svm_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    \n",
    "def get_params_range(model_name) :\n",
    "    if 'xgb' in model_name : \n",
    "        return {'learning_rate': (1, 12),\n",
    "                'booster' : (1, 3),\n",
    "                'gamma' : (0, 5),\n",
    "                'max_depth': (3, 10),\n",
    "                'min_child_weight': (1, 1),\n",
    "                'max_delta_weight': (1, 12),\n",
    "                'rate_drop': (0, 1)}\n",
    "    elif 'knn' in model_name : \n",
    "        return {'weight': (0, 1),\n",
    "                'algorithm' : (0, 3),\n",
    "                'leaf_size' : (5, 50),\n",
    "                'p': (1, 2),}\n",
    "    elif 'svm' in model_name : \n",
    "        return {'C': (0.01, 1), 'random_state' : (0, 5)}\n",
    "    elif 'lgbm' in model_name :\n",
    "        return {'min_child_weight': (0.01, 1),\n",
    "              'learning_rate': (1, 10),\n",
    "              'max_depth': (-1, 10),\n",
    "              'num_leaves': (5, 20)}\n",
    "\n",
    "def reset_model(model_name, params=None) :\n",
    "    if 'xgb' in model_name :\n",
    "        return XGBClassifier(**xgboost_params) if params is None else XGBClassifier(**params)\n",
    "    elif 'knn' in model_name :\n",
    "        return KNeighborsClassifier(**knn_params)\n",
    "    elif 'svm' in model_name :\n",
    "        return svm.LinearSVC(**svm_params) if params is None else svm.LinearSVC(**params)\n",
    "    else :\n",
    "        return LGBMClassifier(**lgbm_params) if params is None else LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = ['xgboost', 'lgbm', 'knn', 'svm']\n",
    "model_idx = 1\n",
    "model_name = model_name_list[model_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predicted Coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cut = 50  \n",
    "y_cut = 100 \n",
    "\n",
    "old_origin_img = cv2.imread('../image/map.png',0)\n",
    "crop = old_origin_img[y_cut:318, x_cut:927]\n",
    "crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "x_coord_list = []\n",
    "y_coord_list = []\n",
    "pci_list = []\n",
    "for lon in range(0, crop.shape[1]) :\n",
    "    for lat in range(0, crop.shape[0]) :\n",
    "        x_coord_list.append(x_cut + lon)\n",
    "        y_coord_list.append(y_cut + lat)\n",
    "        \n",
    "all_x_pci = pd.DataFrame({'location_x':x_coord_list, 'location_y':y_coord_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "loc_df = pci_data[['location_x', 'location_y', 'PCI']]\n",
    "def get_pci(x, y) :\n",
    "    distance = lambda d: math.hypot(abs(x-d[0]), abs(y-d[1]))\n",
    "    loc_df[\"d\"] = loc_df.apply(distance, axis=1)\n",
    "    return loc_df.loc[loc_df.d.idxmin()].PCI\n",
    "\n",
    "class target() :\n",
    "    def optimize(self, x, y) :\n",
    "        if self.bayes_opt is None or self.bayes_opt.X is None:\n",
    "            return 1000\n",
    "\n",
    "        bo = self.bayes_opt\n",
    "        bo.gp.fit(bo.X, bo.Y)\n",
    "        mu, sigma = bo.gp.predict(all_x_pci.values, return_std=True)\n",
    "        return -np.mean(sigma)\n",
    "\n",
    "def posterior(bo, x):\n",
    "    bo.gp.fit(bo.X, bo.Y)\n",
    "    mu, sigma = bo.gp.predict(x, return_std=True)\n",
    "    plot(sigma)\n",
    "    plt.show()\n",
    "    return mu, sigma\n",
    "\n",
    "def plot_gp(bo, x, curr_x_train, curr_y_train, set_val, model, show_sigma_map=False):\n",
    "    path = \"../results/predicted/pci/real_%s_set_%d.png\" % (model, set_val)\n",
    "    background = get_map_image()\n",
    "    p_color = [pci_decode[y] for y in curr_y_train]\n",
    "    p_color = [pci_color_dict[y] if y in pci_color_dict else (255, 255, 255) for y in p_color]\n",
    "    b = visualize(background, curr_x_train['location_x'].astype(int), curr_x_train['location_y'].astype(int), \n",
    "                  p_color, path, adjustment=True)\n",
    "\n",
    "    if show_sigma_map :\n",
    "        normalize_sigma = matplotlib.colors.Normalize(vmin=min(sigma), vmax=max(sigma))\n",
    "        mu_map = [cmap(normalize_sigma(value))[:3] for value in mu_sigma]\n",
    "        mu_map = [[int(x*255) for x in value] for value in mu_map]    \n",
    "        a=visualize_all_location_heatmap(a, x_coord_view, y_coord_view, mu_map, \n",
    "                                         cmap, normalize_sigma, filename=None,\n",
    "                                         size=1, figsize=(20,10), adjustment=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "    1 | 00m00s | \u001b[35m1000.00000\u001b[0m | \u001b[32m 532.4990\u001b[0m | \u001b[32m 106.4831\u001b[0m | \n",
      "    2 | 00m00s |   -0.99999 |  670.3375 |  199.1328 | \n",
      "    3 | 00m00s |   -0.99998 |  304.8326 |  240.8643 | \n",
      "    4 | 00m00s |   -0.99998 |  497.4850 |  160.4317 | \n",
      "    5 | 00m00s |   -0.99997 |  832.2215 |  246.7473 | \n",
      "    6 | 00m00s |   -0.99996 |  835.1527 |  228.2172 | \n",
      "    7 | 00m00s |   -0.99968 |  160.0127 |  105.2041 | \n",
      "    8 | 00m00s |   -0.99965 |  231.5448 |  221.2713 | \n",
      "    9 | 00m00s |   -0.99968 |   95.0853 |  156.2578 | \n",
      "   10 | 00m00s |   -0.99346 |  436.1494 |  190.0770 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "   11 | 00m00s |   -0.99915 |  191.6835 |  118.1789 | \n",
      "   12 | 00m00s |   -0.99907 |  685.8851 |  220.6383 | \n",
      "   13 | 00m00s |   -0.99898 |  759.7297 |  189.2084 | \n",
      "   14 | 00m00s |   -0.99890 |  689.5051 |  205.3176 | \n",
      "   15 | 00m00s |   -0.99879 |   74.1186 |  293.2435 | \n",
      "   16 | 00m00s |   -0.99870 |  284.0895 |  112.8271 | \n",
      "   17 | 00m00s |   -0.99861 |  124.0220 |  283.6385 | \n",
      "   18 | 00m00s |   -0.99853 |  907.3729 |  206.2395 | \n",
      "   19 | 00m00s |   -0.99844 |  525.9184 |  277.4743 | \n",
      "   20 | 00m00s |   -0.99835 |  889.5909 |  282.8063 | \n",
      "   21 | 00m00s |   -0.99827 |  642.3598 |  141.8439 | \n",
      "   22 | 00m00s |   -0.99818 |   83.1540 |  245.8494 | \n",
      "   23 | 00m00s |   -0.99809 |  146.9658 |  227.5656 | \n",
      "   24 | 00m00s |   -0.99801 |  849.4231 |  203.5465 | \n",
      "   25 | 00m00s |   -0.99792 |  911.0347 |  162.5082 | \n",
      "   26 | 00m00s |   -0.99783 |  721.3587 |  147.3591 | \n",
      "   27 | 00m00s |   -0.99775 |  749.2406 |  200.1970 | \n",
      "   28 | 00m00s |   -0.99761 |  231.8516 |  195.4195 | \n",
      "   29 | 00m00s |   -0.99752 |  311.4348 |  294.0007 | \n",
      "   30 | 00m00s |   -0.99743 |  655.4158 |  163.9583 | \n",
      "   31 | 00m00s |   -0.99734 |  229.9200 |  209.7774 | \n",
      "   32 | 00m00s |   -0.96681 |  756.8032 |  181.2018 | \n",
      "   33 | 00m00s |   -0.96518 |  285.9863 |  128.3427 | \n",
      "   34 | 00m00s |   -0.96360 |  217.3156 |  174.1325 | \n",
      "   35 | 00m00s |   -0.96212 |  260.0582 |  162.4782 | \n",
      "   36 | 00m00s |   -0.96078 |  651.3951 |  197.7339 | \n",
      "   37 | 00m01s |   -0.95915 |  501.9965 |  299.8920 | \n",
      "   38 | 00m00s |   -0.95778 |   95.0561 |  215.5687 | \n",
      "   39 | 00m00s |   -0.95630 |  663.7406 |  286.6141 | \n",
      "   40 | 00m00s |   -0.95507 |   61.6919 |  222.3986 | \n",
      "   41 | 00m00s |   -0.95360 |  512.8978 |  190.5508 | \n",
      "   42 | 00m00s |   -0.95208 |  255.5382 |  151.9782 | \n",
      "   43 | 00m00s |   -0.95053 |   92.1261 |  216.3475 | \n",
      "   44 | 00m00s |   -0.94916 |  733.2160 |  304.0027 | \n",
      "   45 | 00m00s |   -0.94793 |  497.1188 |  135.5287 | \n",
      "   46 | 00m00s |   -0.97357 |  316.4998 |  182.7692 | \n",
      "   47 | 00m00s |   -0.97296 |   86.4978 |  282.8888 | \n",
      "   48 | 00m00s |   -0.97202 |  192.0104 |  205.8806 | \n",
      "   49 | 00m00s |   -0.97137 |  750.5599 |  256.3746 | \n",
      "   50 | 00m00s |   -0.97074 |  216.8911 |  121.2432 | \n",
      "   51 | 00m00s |   -0.97000 |  835.2548 |  140.0805 | \n",
      "   52 | 00m00s |   -0.96937 |  274.9533 |  265.0504 | \n",
      "   53 | 00m00s |   -0.96873 |  876.0624 |  295.1737 | \n",
      "   54 | 00m00s |   -0.96784 |  417.3883 |  273.1587 | \n",
      "   55 | 00m00s |   -0.96721 |  440.5952 |  261.5004 | \n",
      "   56 | 00m00s |   -0.96647 |  248.9162 |  251.2742 | \n",
      "   57 | 00m00s |   -0.96574 |  409.3865 |  178.5090 | \n",
      "   58 | 00m00s |   -0.96503 |  377.7747 |  187.8222 | \n",
      "   59 | 00m00s |   -0.96435 |  375.4191 |  110.4182 | \n",
      "   60 | 00m00s |   -0.96370 |  585.1444 |  190.1304 | \n"
     ]
    }
   ],
   "source": [
    "random = 3\n",
    "t = target()\n",
    "bo3 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "                                        'y': (min(y_coord_list), max(y_coord_list))},\n",
    "                           random_state=random, \n",
    "                           verbose=1)\n",
    "t.bayes_opt = bo3\n",
    "\n",
    "iterations = 500\n",
    "gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "bo3.maximize(init_points=10, n_iter=iterations, acq=\"ei\", xi=1e+1, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.2 xgboost 0.23773584905660372\n",
      "1 0.5 xgboost 0.23773584905660372\n",
      "1 0.7 xgboost 0.23773584905660372\n",
      "2 0.2 xgboost 0.29846153846153844\n",
      "2 0.5 xgboost 0.29846153846153844\n",
      "2 0.7 xgboost 0.29846153846153844\n",
      "3 0.2 xgboost 0.31512605042016806\n",
      "3 0.5 xgboost 0.31512605042016806\n",
      "3 0.7 xgboost 0.31512605042016806\n",
      "4 0.2 xgboost 0.26363636363636367\n",
      "4 0.5 xgboost 0.24884792626728114\n",
      "4 0.7 xgboost 0.24884792626728114\n",
      "5 0.2 xgboost 0.33333333333333337\n",
      "5 0.5 xgboost 0.3228699551569507\n",
      "5 0.7 xgboost 0.3228699551569507\n",
      "6 0.2 xgboost 0.3492063492063492\n",
      "6 0.5 xgboost 0.3492063492063492\n",
      "6 0.7 xgboost 0.3492063492063492\n",
      "7 0.2 xgboost 0.39534883720930236\n",
      "7 0.5 xgboost 0.39534883720930236\n",
      "7 0.7 xgboost 0.39534883720930236\n",
      "8 0.2 xgboost 0.37254901960784315\n",
      "8 0.5 xgboost 0.37254901960784315\n",
      "8 0.7 xgboost 0.37254901960784315\n",
      "9 0.2 xgboost 0.31666666666666665\n",
      "9 0.5 xgboost 0.31666666666666665\n",
      "9 0.7 xgboost 0.31666666666666665\n",
      "10 0.2 xgboost 0.2890995260663507\n",
      "10 0.5 xgboost 0.23188405797101452\n",
      "10 0.7 xgboost 0.23188405797101452\n",
      "11 0.2 xgboost 0.2710280373831776\n",
      "11 0.5 xgboost 0.23696682464454977\n",
      "11 0.7 xgboost 0.23696682464454977\n",
      "12 0.2 xgboost 0.3027522935779816\n",
      "12 0.5 xgboost 0.2710280373831776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f3fd295cb503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                          verbose=0)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mxgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#             print(xgbBO.res['max']['max_params'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                             **self._acqkw)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# Keep track of total number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36macq_max\u001b[0;34m(ac, gp, y_max, bounds, random_state, n_warmup, n_iter)\u001b[0m\n\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                        method=\"L-BFGS-B\")\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Store it if better than previous minimum(maximum).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 603\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_try\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Find the minimum of minus the acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),\n\u001b[0m\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36mutility\u001b[0;34m(self, x, gp, y_max)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ucb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ei'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m_ucb\u001b[0;34m(x, gp, kappa)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkappa\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0my_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predict based on GP posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mK_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Line 4 (y_mean = f_star)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_mean\u001b[0m  \u001b[0;31m# undo normal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1322\u001b[0m                     \"Gradient can only be evaluated when Y is None.\")\n\u001b[1;32m   1323\u001b[0m             dists = cdist(X / length_scale, Y / length_scale,\n\u001b[0;32m-> 1324\u001b[0;31m                           metric='euclidean')\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             XA, XB, typ, kwargs = _validate_cdist_input(XA, XB, mA, mB, n,\n\u001b[0;32m-> 2656\u001b[0;31m                                                         metric_name, **kwargs)\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[0;31m# get cdist wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             cdist_fn = getattr(_distance_wrap,\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_cdist_input\u001b[0;34m(XA, XB, mA, mB, n, metric_name, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_validate_cdist_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetric_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# get supported types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_dict = {}\n",
    "for set_val in demo_config[6] :\n",
    "    for percentage in [0.2, 0.5, 0.7] :\n",
    "        for model_name in model_name_list :\n",
    "            curr_pci_data = pci_data[pci_data.set == set_val]\n",
    "            iterations = int(percentage*len(curr_pci_data)) + 5\n",
    "\n",
    "            temp = curr_pci_data.copy()\n",
    "            temp2 = pd.DataFrame(columns=temp.columns)\n",
    "            for x in bo3.X[:iterations] :\n",
    "                distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "                temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "                temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "            temp3 = curr_pci_data[~curr_pci_data.index.isin(temp2.index)]\n",
    "\n",
    "            curr_x_train = temp2.drop([\"PCI\", \"d\"], axis=1)\n",
    "            curr_y_train = temp2.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    "            curr_x_test = temp3.drop(\"PCI\", axis=1)\n",
    "            curr_y_test = temp3.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    "\n",
    "            t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "            xgbBO = BayesianOptimization(t.evaluate, \n",
    "                                         get_params_range(model_name),\n",
    "                                         random_state = random, \n",
    "                                         verbose=0)\n",
    "\n",
    "            xgbBO.maximize(init_points=5, n_iter=3)\n",
    "#             print(xgbBO.res['max']['max_params'])\n",
    "            params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "            if 'lgbm' in model_name :\n",
    "                params['min_data_in_bin']=1\n",
    "                params['min_data']=1\n",
    "\n",
    "            model = reset_model(model_name, params)\n",
    "            model.fit(curr_x_train, curr_y_train)\n",
    "            pickle.dump(model, open(\"db/%s_%s_%d_bayesian_independent_%s.pickle.dat\" % \\\n",
    "                                    ('PCI', model_name, percentage*100, set_val), \"wb\"))\n",
    "\n",
    "        # for set_val in demo_config[6] :\n",
    "            y_pci_pred = model.predict(curr_x_test)\n",
    "            predictions = [round(value) for value in y_pci_pred]\n",
    "            accuracy = 1-accuracy_score(curr_y_test, predictions)\n",
    "            print(set_val, percentage, model_name, accuracy)\n",
    "            acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# all_curr_x_train, all_curr_y_train = pd.DataFrame(), []\n",
    "# all_curr_x_test, all_curr_y_test = pd.DataFrame(), []\n",
    "# all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_pci_data = pci_data[pci_data.set == set_val]\n",
    "#     iterations = int(0.2*len(curr_pci_data))\n",
    "\n",
    "#     temp = curr_pci_data.copy()\n",
    "#     temp2 = pd.DataFrame(columns=temp.columns)\n",
    "#     for x in bo3.X[:iterations] :\n",
    "#         distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "#         temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "#         temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "#     temp3 = curr_pci_data[~curr_pci_data.index.isin(temp2.index)]\n",
    "\n",
    "#     curr_x_train = temp2.drop([\"PCI\", \"d\"], axis=1)\n",
    "#     curr_y_train = temp2.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    "#     curr_x_test = temp3.drop(\"PCI\", axis=1)\n",
    "#     curr_y_test = temp3.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    "\n",
    "#     all_curr_x_train = all_curr_x_train.append(curr_x_train)\n",
    "#     all_curr_y_train += curr_y_train \n",
    "#     all_curr_x_test = all_curr_x_test.append(curr_x_test)\n",
    "#     all_curr_y_test += curr_y_test\n",
    "#     all_curr_x_test_dict[set_val] = curr_x_test\n",
    "#     all_curr_y_test_dict[set_val] = curr_y_test  \n",
    "\n",
    "# #     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "# t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "# xgbBO = BayesianOptimization(t.evaluate, \n",
    "#                              get_params_range(model_name),\n",
    "#                              random_state = random, \n",
    "#                              verbose=1)\n",
    "\n",
    "# xgbBO.maximize(init_points=5, n_iter=15)\n",
    "# print(xgbBO.res['max']['max_params'])\n",
    "# params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "# # params = lgbm_params\n",
    "# params['min_data_in_bin']=1\n",
    "# params['min_data']=1\n",
    "    \n",
    "# model = reset_model(model_name, params)\n",
    "# model.fit(curr_x_train, curr_y_train)\n",
    "# pickle.dump(model, open(\"db/%s_%s_bayesian_baseline_%s.pickle.dat\" % ('PCI', model_name, set_val), \"wb\"))\n",
    "\n",
    "# for set_val in demo_config[6] :\n",
    "#     y_pci_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "#     predictions = [round(value) for value in y_pci_pred]\n",
    "#     accuracy = accuracy_score(all_curr_y_test_dict[set_val], predictions)\n",
    "#     acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), accuracy]\n",
    "#     print(1-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# all_curr_x_train_dict, all_curr_y_train_dict = {}, {}\n",
    "# all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_pci_data = pci_data[pci_data.set == set_val]\n",
    "#     iterations = int(0.7*len(curr_pci_data)) + 5\n",
    "\n",
    "#     temp = curr_pci_data.copy()\n",
    "#     temp2 = pd.DataFrame(columns=temp.columns)\n",
    "#     for x in bo3.X[:iterations] :\n",
    "#         distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "#         temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "#         temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "#     curr_x_train = temp2.drop([\"PCI\", \"d\"], axis=1)\n",
    "#     curr_y_train = temp2.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    " \n",
    "#     all_curr_x_train_dict[set_val] = curr_x_train\n",
    "#     all_curr_y_train_dict[set_val] = curr_y_train  \n",
    "#     all_curr_x_test_dict[set_val] = curr_pci_data.drop(\"PCI\", axis=1)\n",
    "#     all_curr_y_test_dict[set_val] = curr_pci_data.PCI.apply(lambda x : pci_encode[x]).values.tolist()\n",
    "# #     print(set_val, len(curr_x_train), len(curr_x_test))\n",
    "\n",
    "# #     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "# #     t = get_target(model_name, all_curr_x_train, all_curr_y_train, all_curr_x_test, all_curr_y_test)\n",
    "# #     xgbBO = BayesianOptimization(t.evaluate, \n",
    "# #                                  get_params_range(model_name),\n",
    "# #                                  random_state = random, \n",
    "# #                                  verbose=0)\n",
    "\n",
    "# #     xgbBO.maximize(init_points=5, n_iter=3)\n",
    "# #     print(xgbBO.res['max']['max_params'])\n",
    "# #     params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "#     params = lgbm_params\n",
    "#     params['min_data_in_bin']=1\n",
    "#     params['min_data']=1\n",
    "    \n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_x_train, curr_y_train = pd.DataFrame(), []\n",
    "#     for k in all_curr_x_train_dict :\n",
    "#         if k != set_val :\n",
    "#             curr_x_train = curr_x_train.append(all_curr_x_train_dict[k])\n",
    "#             curr_y_train += all_curr_y_train_dict[k]\n",
    "    \n",
    "#     model = reset_model(model_name, params)\n",
    "#     model.fit(curr_x_train, curr_y_train)\n",
    "#     pickle.dump(model, open(\"db/%s_%s_bayesian_transfer_%s.pickle.dat\" % ('PCI', model_name, set_val), \"wb\"))\n",
    "\n",
    "#     y_pci_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "#     predictions = [round(value) for value in y_pci_pred]\n",
    "#     accuracy = accuracy_score(all_curr_y_test_dict[set_val], predictions)\n",
    "#     acc_dict[set_val] = [len(curr_x_train), len(all_curr_y_test_dict[set_val]), accuracy]\n",
    "#     print(1-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
