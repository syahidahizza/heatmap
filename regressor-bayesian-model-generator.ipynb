{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from predictionhelper.ipynb\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylab import *\n",
    "\n",
    "import pickle\n",
    "import loadnotebook\n",
    "from predictionhelper import *\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sklearn.metrics as metric\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113700\n"
     ]
    }
   ],
   "source": [
    "sets = [x for x in range(1, 34)]\n",
    "demo_config = {6 : sets}\n",
    "\n",
    "df_all_data = get_data(config=demo_config, pure=True, refresh=False).reset_index(drop=True)\n",
    "print(len(df_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_columns = [\"RSRP\", \"RSRQ\", \"SNR\"]\n",
    "pred_index = 2\n",
    "pred_col = prediction_columns[pred_index]\n",
    "group = ['location_x', 'location_y', 'priority', 'set', 'PCI']\n",
    "group2 = ['location_x', 'location_y', 'priority', 'set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = [c for c in df_all_data if \"beam\" in c] + prediction_columns[pred_index+1:]\n",
    "df_data = df_all_data.drop(dropped_columns, axis=1)\n",
    "df_data = df_data[df_data[\"PCI\"].isin(whitelist_PCI)]\n",
    "df_data = df_data.dropna()\n",
    "\n",
    "df_data = merge_agg(df_data, group, pred_col, ['count'])\n",
    "new_cols = [x+\"_mean\" for x in prediction_columns[:pred_index+1]]\n",
    "df_data = merge_agg(df_data, group, prediction_columns[:pred_index+1], ['mean'], new_cols)\n",
    "idx = df_data.groupby(group2)['count'].transform(max) == df_data['count']\n",
    "df_data = df_data[idx].reset_index(drop=True)\n",
    "\n",
    "for i in range(pred_index+1) :\n",
    "    df_data[prediction_columns[i]] = df_data[prediction_columns[i] + \"_mean\"]\n",
    "    \n",
    "df_data = df_data.drop(new_cols+['count'], axis=1)\n",
    "df_data = df_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4674 2027\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = pd.DataFrame(), pd.DataFrame()\n",
    "data_train_dict, data_test_dict = {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = train_test_split(df_data[df_data.set==s], test_size=0.3, random_state=32)\n",
    "        data_train = data_train.append(a)\n",
    "        data_test = data_test.append(b)   \n",
    "        data_train_dict[(p, s)] = a\n",
    "        data_test_dict[(p, s)] = b\n",
    "print(len(data_train), len(data_test))\n",
    "\n",
    "exclude_train_col = ['priority', pred_col]\n",
    "x_train = data_train.drop(exclude_train_col, axis=1)\n",
    "y_train = np.array(data_train[pred_col].values.tolist())\n",
    "x_test = data_test.drop(exclude_train_col, axis=1)\n",
    "y_test = np.array(data_test[pred_col].values.tolist())\n",
    "\n",
    "x_train_dict, y_train_dict, x_test_dict, y_test_dict = {}, {}, {}, {}\n",
    "for p in demo_config :\n",
    "    for s in demo_config[p] :\n",
    "        a, b = data_train_dict[(p,s)], data_test_dict[(p,s)]\n",
    "        x_train_dict[(p, s)] = a.drop(exclude_train_col, axis=1)\n",
    "        y_train_dict[(p, s)] = np.array(a[pred_col].values.tolist())\n",
    "        x_test_dict[(p, s)] = b.drop(exclude_train_col, axis=1)\n",
    "        y_test_dict[(p, s)] = np.array(b[pred_col].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_params = {'kernel':'rbf'}\n",
    "\n",
    "class svm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'kernel':'poly', 'degree':3, 'gamma':'auto', 'coef0':0.0, 'tol':0.001}\n",
    "        params = {'kernel':'rbf', 'degree':3, 'gamma':'auto', 'coef0':0.0, 'tol':0.001}\n",
    "        params['C'] = param['C'] if param['C'] > 0 else 0.1\n",
    "        params['epsilon'] = param['epsilon'] if param['epsilon'] > 0 else 0.001\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, degree=3, gamma='auto_deprecated', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1):\n",
    "        params = {'degree':degree, 'gamma':gamma, 'coef0':coef0, 'tol':tol, 'C':C, 'epsilon':epsilon}\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model =svm.SVR(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# st = svm_target(x_train, y_train, \n",
    "#                 x_test, y_test)\n",
    "# sBO = BayesianOptimization(st.evaluate, {'C': (0.001, 1), 'epsilon' : (0, 0.1)},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# sBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_params = {'kernel':'rbf'}\n",
    "# model = svm.SVR(**svm_params)\n",
    "# model.fit(x_train, y_train)\n",
    "# y_pred = model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_params_list = [\n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'distance',\n",
    " 'algorithm': 'brute',\n",
    " 'leaf_size': 5,\n",
    " 'p': 1}, \n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'uniform',\n",
    " 'algorithm': 'kd_tree',\n",
    " 'leaf_size': 49,\n",
    " 'p': 1},\n",
    "{'n_neighbors': len(whitelist_PCI),\n",
    " 'weights': 'uniform',\n",
    " 'algorithm': 'kd_tree',\n",
    " 'leaf_size': 9,\n",
    " 'p': 1}\n",
    "]\n",
    "\n",
    "knn_params = knn_params_list[pred_index]\n",
    "\n",
    "class knn_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.weights = ['uniform', 'distance']\n",
    "        self.algorithms = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'n_neighbors':7}\n",
    "        params['weights'] = self.weights[int(param['weight'])]\n",
    "        params['algorithm'] = self.algorithms[int(param['algorithm'])]\n",
    "        params['leaf_size'] = int(param['leaf_size'])\n",
    "        params['p'] = int(param['p'])\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, weight, algorithm, leaf_size=100, p=2):\n",
    "        params = {}\n",
    "        params['weight'] = weight\n",
    "        params['algorithm'] = algorithm\n",
    "        params['leaf_size'] = int(leaf_size)\n",
    "        params['p'] = int(p)\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        model = KNeighborsRegressor(**params)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        y_pred = model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kt = knn_target(x_train, y_train, \n",
    "#                 x_test, y_test)\n",
    "# kBO = BayesianOptimization(kt.evaluate, {'weight': (0, 1),\n",
    "#                                         'algorithm' : (0, 3),\n",
    "#                                         'leaf_size' : (5, 50),\n",
    "#                                         'p': (1, 2),},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# kBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = kt.clean_param(kBO.res['max']['max_params'])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNeighborsRegressor(**knn_params)\n",
    "# model.fit(x_train, y_train)\n",
    "# y_pred = model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgboost_params_list = [\n",
    "# RSRP,\n",
    "{'base_score': 0.5,\n",
    "'booster': 'gbtree',\n",
    "'missing': None,\n",
    "'n_estimators': 100,\n",
    "'n_jobs': 1,\n",
    "'random_state': 1,\n",
    "'reg_lambda': 1,\n",
    "'alpha': 0,\n",
    "'scale_pos_weight': 1,\n",
    "'subsample': 1,\n",
    "'colsample_bytree': 1,\n",
    "'colsample_bylevel': 1,\n",
    "'objective': 'reg:linear',\n",
    "'learning_rate': 0.06926984074036927,\n",
    "'gamma': 43.90712517147065,\n",
    "'max_depth': 9,\n",
    "'min_child_weight': 1,\n",
    "'max_delta_weight': 14,\n",
    "'rate_drop': 0.5865550405019929},\n",
    "# RSRQ\n",
    "{'base_score': 0.5,\n",
    " 'booster': 'dart',\n",
    " 'missing': None,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 1,\n",
    " 'random_state': 1,\n",
    " 'reg_lambda': 1,\n",
    " 'alpha': 0,\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 1,\n",
    " 'objective': 'reg:linear',\n",
    " 'learning_rate': 0.12,\n",
    " 'gamma': 0.0,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 1,\n",
    " 'max_delta_weight': 20,\n",
    " 'rate_drop': 0.0},\n",
    "# SNR\n",
    "{'base_score': 0.5,\n",
    " 'booster': 'dart',\n",
    " 'missing': None,\n",
    " 'n_estimators': 100,\n",
    " 'n_jobs': 1,\n",
    " 'random_state': 1,\n",
    " 'reg_lambda': 1,\n",
    " 'alpha': 0,\n",
    " 'scale_pos_weight': 1,\n",
    " 'subsample': 1,\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 1,\n",
    " 'objective': 'reg:linear',\n",
    " 'learning_rate': 0.12,\n",
    " 'gamma': 0.0,\n",
    " 'max_depth': 12,\n",
    " 'min_child_weight': 1,\n",
    " 'max_delta_weight': 20,\n",
    " 'rate_drop': 0.0}\n",
    "]\n",
    "\n",
    "xgboost_params = xgboost_params_list[pred_index]\n",
    "\n",
    "class xgboost_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        booster_dict = {1:'gbtree', 2:'gblinear', 3:'dart'}\n",
    "        params = {'base_score':0.5, 'booster':'gbtree', 'missing':None, 'n_estimators':100, \n",
    "                  'n_jobs':1, 'random_state':1, 'reg_lambda':1, 'alpha':0, 'scale_pos_weight':1, \n",
    "                  'subsample':1, 'colsample_bytree':1, 'colsample_bylevel':1}\n",
    "        \n",
    "        params['objective'] = 'reg:linear'\n",
    "        params['learning_rate'] = param['learning_rate']/100\n",
    "        params['booster'] = booster_dict[int(param['booster'])]\n",
    "        params['gamma'] = param['gamma']\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_delta_weight'] = int(param['max_delta_weight'])\n",
    "        params['rate_drop'] = param['rate_drop']\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, learning_rate, booster, gamma, max_depth,  \n",
    "                     min_child_weight, max_delta_weight, rate_drop):\n",
    "\n",
    "        params = {}\n",
    "        params['learning_rate'] = learning_rate\n",
    "        params['booster'] = booster\n",
    "        params['gamma'] = gamma\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['max_delta_weight'] = max_delta_weight\n",
    "        params['rate_drop'] = rate_drop\n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        xgb_model = XGBRegressor(**params)\n",
    "        xgb_model.fit(self.x_train, self.y_train)\n",
    "        y_pred = xgb_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xt = xgboost_target(x_train, y_train, x_test, y_test)\n",
    "# xgbBO = BayesianOptimization(xt.evaluate, {'learning_rate': (1, 12),\n",
    "#                                             'booster' : (1, 3),\n",
    "#                                             'gamma' : (0, 50),\n",
    "#                                             'max_depth': (3, 12),\n",
    "#                                             'min_child_weight': (1, 1),\n",
    "#                                             'max_delta_weight': (1, 20),\n",
    "#                                             'rate_drop': (0, 1)},\n",
    "#                             random_state = 1)\n",
    "\n",
    "# xgbBO.maximize(init_points=10, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = xt.clean_param(xgbBO.res['max']['max_params'])\n",
    "# xgb_model = XGBRegressor(**params)\n",
    "# xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = xgb_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = XGBRegressor(**xgboost_params)\n",
    "# xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# y_pred = xgb_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm_params_list = [\n",
    "# RSRP\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 50,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': -1,\n",
    " 'learning_rate': 0.1,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1},\n",
    "# RSRQ\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 19,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': 5,\n",
    " 'learning_rate': 0.0995815310228581,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1},\n",
    "# SNR\n",
    "{'boosting_type': 'gbdt',\n",
    " 'class_weight': None,\n",
    " 'colsample_bytree': 1.0,\n",
    " 'importance_type': 'split',\n",
    " 'min_child_samples': 20,\n",
    " 'min_split_gain': 0.0,\n",
    " 'n_estimators': 100,\n",
    " 'objective': None,\n",
    " 'random_state': 0,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'silent': True,\n",
    " 'subsample': 1.0,\n",
    " 'subsample_for_bin': 200000,\n",
    " 'subsample_freq': 0,\n",
    " 'num_leaves': 32,\n",
    " 'min_child_weight': 0,\n",
    " 'max_depth': 11,\n",
    " 'learning_rate': 0.1,\n",
    " 'min_data_in_bin': 1,\n",
    " 'min_data': 1}\n",
    "]\n",
    "\n",
    "lgbm_params = lgbm_params_list[pred_index]\n",
    "\n",
    "class lgbm_target :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) :\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def clean_param(self, param) :\n",
    "        params = {'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':1.0, \n",
    "                  'importance_type':'split', \n",
    "                  'min_child_samples':20, 'min_split_gain':0.0, 'n_estimators':100, 'objective':None,\n",
    "                  'random_state':0, 'reg_alpha':0.0, 'reg_lambda':0.0, 'silent':True,\n",
    "                  'subsample':1.0, 'subsample_for_bin':200000, 'subsample_freq':0}\n",
    "        params['num_leaves'] = int(param['num_leaves'])\n",
    "        params['min_child_weight'] = int(param['min_child_weight'])\n",
    "        params['max_depth'] = int(param['max_depth'])\n",
    "        params['learning_rate'] = param['learning_rate'] / 100\n",
    "        params['min_data_in_bin'] = 1\n",
    "        params['min_data'] = 1\n",
    "        return params\n",
    "        \n",
    "    def evaluate(self, min_child_weight, learning_rate, max_depth, num_leaves):\n",
    "        params = {'num_leaves':num_leaves, \n",
    "                  'min_child_weight':min_child_weight, \n",
    "                  'max_depth':max_depth, \n",
    "                  'learning_rate':learning_rate}\n",
    "        \n",
    "        params = self.clean_param(params)\n",
    "\n",
    "        lgbm_model = LGBMRegressor(**params)\n",
    "        lgbm_model.fit(self.x_train, self.y_train)\n",
    "        y_pred = lgbm_model.predict(self.x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(self.y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt = lgbm_target(x_train, y_train, x_test, y_test)\n",
    "# lgbmBO = BayesianOptimization(lt.evaluate, {'min_child_weight': (0.01, 1),\n",
    "#                                               'learning_rate': (1, 10),\n",
    "#                                               'max_depth': (-1, 15),\n",
    "#                                               'num_leaves': (5, 50)}, \n",
    "#                              random_state=3)\n",
    "\n",
    "# lgbmBO.maximize(init_points=20, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = lt.clean_param(lgbmBO.res['max']['max_params'])\n",
    "# lgbm_model = LGBMRegressor(**params)\n",
    "# lgbm_model.fit(x_train, y_train)\n",
    "# y_pred = lgbm_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_model = LGBMRegressor(**lgbm_params)\n",
    "# lgbm_model.fit(x_train, y_train)\n",
    "# y_pred = lgbm_model.predict(x_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# mse = metric.mean_squared_error(y_test, predictions)\n",
    "# rmse = math.sqrt(mse)/(max(y_test)-min(y_test))\n",
    "# print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_matrix = np.empty((4, 3, 34))\n",
    "nrmse_matrix[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = ['xgboost', 'lgbm', 'knn', 'svm']\n",
    "\n",
    "def get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test) :\n",
    "    if 'xgb' in model_name : \n",
    "        return xgboost_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    elif 'knn' in model_name : \n",
    "        return knn_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    elif 'lgbm' in model_name : \n",
    "        return lgbm_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    else :\n",
    "        return svm_target(curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "    \n",
    "def get_params_range(model_name) :\n",
    "    if 'xgb' in model_name : \n",
    "        return {'learning_rate': (1, 12),\n",
    "                'booster' : (1, 3),\n",
    "                'gamma' : (0, 5),\n",
    "                'max_depth': (3, 10),\n",
    "                'min_child_weight': (1, 1),\n",
    "                'max_delta_weight': (1, 12),\n",
    "                'rate_drop': (0, 1)}\n",
    "    elif 'knn' in model_name :\n",
    "        return {'weight': (0, 1),\n",
    "                'algorithm' : (0, 3),\n",
    "                'leaf_size' : (5, 50),\n",
    "                'p': (1, 2),}\n",
    "    elif 'lgbm' in model_name :\n",
    "        return {'min_child_weight': (0.01, 1),\n",
    "              'learning_rate': (1, 10),\n",
    "              'max_depth': (-1, 15),\n",
    "              'num_leaves': (5, 50)} \n",
    "    else :\n",
    "        return {'C': (0.01, 1), 'epsilon' : (0.001, 0.1)}\n",
    "    \n",
    "def reset_model(model_name, params=None) :\n",
    "    if 'xgb' in model_name :\n",
    "        return XGBRegressor(**xgboost_params) if params is None else XGBRegressor(**params)\n",
    "    elif 'knn' in model_name :\n",
    "        return KNeighborsRegressor(**knn_params) if params is None else KNeighborsRegressor(**params)\n",
    "    elif 'svm' in model_name :\n",
    "        return svm.SVR(**svm_params) if params is None else svm.SVR(**params)\n",
    "    else :\n",
    "        return LGBMRegressor(**lgbm_params) if params is None else LGBMRegressor(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predicted Coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cut = 50  \n",
    "y_cut = 100 \n",
    "\n",
    "old_origin_img = cv2.imread('../image/map.png',0)\n",
    "crop = old_origin_img[y_cut:318, x_cut:927]\n",
    "crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "x_coord_list = []\n",
    "y_coord_list = []\n",
    "pci_list = []\n",
    "for lon in range(0, crop.shape[1]) :\n",
    "    for lat in range(0, crop.shape[0]) :\n",
    "        x_coord_list.append(x_cut + lon)\n",
    "        y_coord_list.append(y_cut + lat)\n",
    "        \n",
    "all_x_pci = pd.DataFrame({'location_x':x_coord_list, 'location_y':y_coord_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Opt - Exploration - Partial Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "class target() :\n",
    "    def optimize(self, x, y) :\n",
    "        if self.bobo is None or self.bobo.X is None:\n",
    "            return -1000\n",
    "\n",
    "        bo = self.bobo\n",
    "        bo.gp.fit(bo.X, bo.Y)\n",
    "        mu, sigma = bo.gp.predict(all_x_pci.values, return_std=True)\n",
    "        return -mean(sigma)\n",
    "\n",
    "def posterior(bo, x):\n",
    "    bo.gp.fit(bo.X, bo.Y)\n",
    "    mu, sigma = bo.gp.predict(x, return_std=True)\n",
    "    plot(sigma)\n",
    "    plt.show()\n",
    "    return mu, sigma\n",
    "\n",
    "def plot_gp(bo, x, curr_x_train, curr_y_train, set_val, model, show_sigma_map=False):\n",
    "#     background = get_map_image()\n",
    "#     pci_val = [get_pci(d[0], d[1]) for d in bo.X]\n",
    "#     observation_color = [pci_color_dict[y] if y in pci_color_dict else (255, 255, 255) for y in pci_val]\n",
    "#     path = \"../results/predicted/pci/bayesian_%s_set_%d.png\" % (model, set_val)\n",
    "#     a = visualize(background, bo.X[:, 0].astype(int), bo.X[:, 1].astype(int), \n",
    "#                   observation_color, path, adjustment=True)\n",
    "    \n",
    "    path = \"../results/predicted/pci/real_%s_set_%d.png\" % (model, set_val)\n",
    "    background = get_map_image()\n",
    "    p_color = [pci_decode[y] for y in curr_y_train]\n",
    "    p_color = [pci_color_dict[y] if y in pci_color_dict else (255, 255, 255) for y in p_color]\n",
    "    b = visualize(background, curr_x_train['location_x'].astype(int), curr_x_train['location_y'].astype(int), \n",
    "                  p_color, path, adjustment=True)\n",
    "\n",
    "    if show_sigma_map :\n",
    "        normalize_sigma = matplotlib.colors.Normalize(vmin=min(sigma), vmax=max(sigma))\n",
    "        mu_map = [cmap(normalize_sigma(value))[:3] for value in mu_sigma]\n",
    "        mu_map = [[int(x*255) for x in value] for value in mu_map]    \n",
    "        a=visualize_all_location_heatmap(a, x_coord_view, y_coord_view, mu_map, \n",
    "                                         cmap, normalize_sigma, filename=None,\n",
    "                                         size=1, figsize=(20,10), adjustment=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target2() :\n",
    "    def __init__(self, set_val) :\n",
    "        self.set_val = set_val\n",
    "    \n",
    "    def optimize(self, x, y) :\n",
    "        if self.bobo is None or self.bobo.X is None or len(self.bobo.X) < 2:\n",
    "            return -1000\n",
    "\n",
    "        curr_df_data = df_data[df_data.set == self.set_val]\n",
    "\n",
    "        temp = curr_df_data.copy()\n",
    "        temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "    \n",
    "        bo = self.bobo\n",
    "        for x in bo.X :\n",
    "            distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "            temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "            temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "        temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "        temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "        curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "        curr_y_train = np.array(temp2[pred_col].values.tolist())\n",
    "        curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "        curr_y_test = np.array(temp3[pred_col].values.tolist())\n",
    "\n",
    "        params = {'min_child_weight': 0.7151893663724195, 'learning_rate': 4.9382849013642325, \n",
    "                  'max_depth': 7.462318716046472, 'num_leaves': 5.909827884814657,\n",
    "                  'min_data':1, 'min_data_in_bin':1}\n",
    "        t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "        params = t.clean_param(params)\n",
    "\n",
    "        model = reset_model(model_name, params)\n",
    "        model.fit(curr_x_train, curr_y_train)\n",
    "\n",
    "        y_pred = model.predict(curr_x_test)\n",
    "        predictions = [round(value) for value in y_pred]\n",
    "        mse = metric.mean_squared_error(curr_y_test, predictions)\n",
    "        rmse = math.sqrt(mse)\n",
    "        return -1*rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# # for set_val in demo_config[6] :\n",
    "# random = 0\n",
    "# t = target2(2)\n",
    "# bo2 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "#                                         'y': (min(y_coord_list), max(y_coord_list))},\n",
    "#                            random_state=random, \n",
    "#                            verbose=1)\n",
    "# t.bobo = bo2\n",
    "\n",
    "# iterations = 50\n",
    "# gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "# bo2.maximize(init_points=10, n_iter=iterations, acq=\"ei\", xi=1e+2, **gp_params)\n",
    "# #     bo2.maximize(init_points=2, n_iter=iterations, acq=\"ei\", xi=1e-4, **gp_params)\n",
    "# acc_dict[set_val] = bo2.res['max']['max_val']\n",
    "# print(acc_dict[set_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_spec_target_inden = np.array([x for x in acc_dict.values()])\n",
    "# for x in list(bayes_spec_target_inden[:, 2]) :\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target : Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "    1 | 00m00s | \u001b[35m-1000.00000\u001b[0m | \u001b[32m 530.7606\u001b[0m | \u001b[32m 230.7997\u001b[0m | \n",
      "    2 | 00m00s | \u001b[35m  -0.99999\u001b[0m | \u001b[32m 676.5059\u001b[0m | \u001b[32m 218.2397\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "    3 | 00m00s | \u001b[35m  -0.99814\u001b[0m | \u001b[32m 689.4791\u001b[0m | \u001b[32m 189.5332\u001b[0m | \n",
      "    4 | 00m00s | \u001b[35m  -0.98018\u001b[0m | \u001b[32m 518.5220\u001b[0m | \u001b[32m 146.1513\u001b[0m | \n",
      "    5 | 00m00s |   -0.98997 |   72.7881 |  228.1264 | \n",
      "    6 | 00m00s |   -0.98748 |  617.4562 |  261.6594 | \n",
      "    7 | 00m00s |   -0.98435 |  837.4735 |  285.0548 | \n"
     ]
    }
   ],
   "source": [
    "random = 0\n",
    "t = target()\n",
    "bo2 = BayesianOptimization(t.optimize, {'x': (min(x_coord_list), max(x_coord_list)), \n",
    "                                        'y': (min(y_coord_list), max(y_coord_list))},\n",
    "                           random_state=random, \n",
    "                           verbose=1)\n",
    "t.bobo = bo2\n",
    "\n",
    "iterations = 500\n",
    "gp_params = {\"alpha\": 1e-5, \"n_restarts_optimizer\": 3, 'random_state':random}\n",
    "bo2.maximize(init_points=2, n_iter=iterations, acq=\"ei\", xi=1e+2, **gp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSRQ 1 0.2 xgboost 0.10154088142044349\n",
      "RSRQ 1 0.2 lgbm 0.10799424386504293\n",
      "RSRQ 1 0.2 knn 0.11295822232377023\n",
      "RSRQ 1 0.2 svm 0.11295822232377023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-658e72b7dd94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                       verbose=0)\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                             **self._acqkw)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# Keep track of total number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36macq_max\u001b[0;34m(ac, gp, y_max, bounds, random_state, n_warmup, n_iter)\u001b[0m\n\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                        method=\"L-BFGS-B\")\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Store it if better than previous minimum(maximum).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 603\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_try\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Find the minimum of minus the acquisition function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),\n\u001b[0m\u001b[1;32m     58\u001b[0m                        \u001b[0mx_try\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                        \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36mutility\u001b[0;34m(self, x, gp, y_max)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ucb'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ei'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/bayes_opt/helpers.py\u001b[0m in \u001b[0;36m_ucb\u001b[0;34m(x, gp, kappa)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ucb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkappa\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/installdir/miniconda3/envs/stock/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_std, return_cov)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Predict based on GP posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mK_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Line 4 (y_mean = f_star)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_train_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_mean\u001b[0m  \u001b[0;31m# undo normal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_cov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_dict = {}\n",
    "for set_val in demo_config[6] :\n",
    "    for percentage in [0.2, 0.5, 0.7] :\n",
    "        for model_name in model_name_list :\n",
    "            curr_df_data = df_data[df_data.set == set_val]\n",
    "            iterations = int(percentage*len(curr_df_data))\n",
    "\n",
    "            temp = curr_df_data.copy()\n",
    "            temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "\n",
    "            for x in bo2.X[:iterations] :\n",
    "                distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "                temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "                temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "            temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "            temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "            curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "\n",
    "            curr_y_train = np.array(temp2[pred_col].values.tolist())\n",
    "            curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "            curr_y_test = np.array(temp3[pred_col].values.tolist())\n",
    "\n",
    "            t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "            bo = BayesianOptimization(t.evaluate, \n",
    "                                      get_params_range(model_name),\n",
    "                                      random_state = random, \n",
    "                                      verbose=0)\n",
    "\n",
    "            bo.maximize(init_points=5, n_iter=1)\n",
    "            params = t.clean_param(bo.res['max']['max_params'])\n",
    "\n",
    "            model = reset_model(model_name, params)\n",
    "            model.fit(curr_x_train, curr_y_train)\n",
    "\n",
    "            y_pred = model.predict(curr_x_test)\n",
    "            predictions = [round(value) for value in y_pred]\n",
    "            mse = metric.mean_squared_error(curr_y_test, predictions)\n",
    "            nrmse = math.sqrt(mse) / (max(curr_y_test) - min(curr_y_test))\n",
    "            print(pred_col, set_val, percentage, model_name, nrmse)\n",
    "            acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), nrmse]\n",
    "            pickle.dump(model, open(\"db/%s_%s_%d_bayesian_independent_%s.pickle.dat\" % \\\n",
    "                                    (pred_col, model_name, percentage*100, set_val), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# all_curr_x_train, all_curr_y_train = pd.DataFrame(), []\n",
    "# all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_df_data = df_data[df_data.set == set_val]\n",
    "#     iterations = int(0.2*len(curr_df_data))\n",
    "\n",
    "#     temp = curr_df_data.copy()\n",
    "#     temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "#     for x in bo2.X[:iterations] :\n",
    "#         distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "#         temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "#         temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "# #     trouble_col = ['PCI', 'Power_37', 'Power_38', 'Power_39', 'Power_40', 'Power_41', 'Power_42', 'set']\n",
    "# #     temp2 = temp2.astype({x:'int' for x in trouble_col})\n",
    "#     temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "#     temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "# #     curr_x_train = temp2.drop([\"d\"] + exclude_train_col, axis=1)\n",
    "#     curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "#     curr_y_train = temp2[pred_col].values.tolist()\n",
    "#     curr_x_test = temp3.drop(exclude_train_col, axis=1)\n",
    "#     curr_y_test = temp3[pred_col].values.tolist()\n",
    "    \n",
    "#     all_curr_x_train = all_curr_x_train.append(curr_x_train)\n",
    "#     all_curr_y_train += curr_y_train \n",
    "#     all_curr_x_test_dict[set_val] = curr_x_test\n",
    "#     all_curr_y_test_dict[set_val] = curr_y_test  \n",
    "\n",
    "# #     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "# t = get_target(model_name, all_curr_x_train, all_curr_y_train, all_curr_x_test, all_curr_y_test)\n",
    "# xgbBO = BayesianOptimization(t.evaluate, \n",
    "#                              get_params_range(model_name),\n",
    "#                              random_state = random, \n",
    "#                              verbose=0)\n",
    "\n",
    "# xgbBO.maximize(init_points=5, n_iter=3)\n",
    "# print(xgbBO.res['max']['max_params'])\n",
    "# params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "# # params = lgbm_params\n",
    "# # params['min_data_in_bin']=1\n",
    "# # params['min_data']=1\n",
    "    \n",
    "# model = reset_model(model_name, params)\n",
    "# model.fit(curr_x_train, curr_y_train)\n",
    "# pickle.dump(model, open(\"db/%s_%s_bayesian_%s.pickle.dat\" % (pred_col, model_name, set_val), \"wb\"))\n",
    "\n",
    "# for set_val in demo_config[6] :\n",
    "#     y_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "#     predictions = [round(value) for value in y_pred]\n",
    "#     mse = metric.mean_squared_error(all_curr_y_test_dict[set_val], predictions)\n",
    "#     rmse = math.sqrt(mse)\n",
    "# #     print(rmse)    \n",
    "#     acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), rmse]\n",
    "#     pickle.dump(model, open(\"db/%s_%s_bayesian_baseline_set_%s.pickle.dat\" % (pred_col, model_name, s), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# acc_dict = {}\n",
    "# all_curr_x_train_dict, all_curr_y_train_dict = {}, {}\n",
    "# all_curr_x_test_dict, all_curr_y_test_dict = {}, {}\n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_df_data = df_data[df_data.set == set_val]\n",
    "#     iterations = int(0.2*len(curr_df_data))\n",
    "\n",
    "#     temp = curr_df_data.copy()\n",
    "#     temp2 = pd.DataFrame(columns=curr_df_data.columns)\n",
    "#     for x in bo2.X[:iterations] :\n",
    "#         distance = lambda d: math.hypot(abs(x[0]-d[0]), abs(x[1]-d[1]))\n",
    "#         temp[\"d\"] = temp.apply(distance, axis=1)\n",
    "#         temp2 = temp2.append(temp.loc[temp.d.idxmin()])\n",
    "\n",
    "# #     trouble_col = ['PCI', 'Power_37', 'Power_38', 'Power_39', 'Power_40', 'Power_41', 'Power_42', 'set']\n",
    "# #     temp2 = temp2.astype({x:'int' for x in trouble_col})\n",
    "#     temp3 = curr_df_data[~curr_df_data.index.isin(temp2.index)]\n",
    "#     temp2 = curr_df_data[~curr_df_data.index.isin(temp3.index)]\n",
    "\n",
    "# #     curr_x_train = temp2.drop([\"d\"] + exclude_train_col, axis=1)\n",
    "#     curr_x_train = temp2.drop(exclude_train_col, axis=1)\n",
    "#     curr_y_train = temp2[pred_col].values.tolist()\n",
    "    \n",
    "#     all_curr_x_train_dict[set_val] = curr_x_train\n",
    "#     all_curr_y_train_dict[set_val] = curr_y_train  \n",
    "#     all_curr_x_test_dict[set_val] = curr_df_data.drop(exclude_train_col, axis=1)\n",
    "#     all_curr_y_test_dict[set_val] = curr_df_data[pred_col].values.tolist()  \n",
    "\n",
    "# #     plot_gp(bo2, all_x_pci.values, curr_x_train, curr_y_train, set_val, \"xgboost\")\n",
    "    \n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':4.2522, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "# #     params = {'learning_rate' : 0.03, 'max_depth' : 9, 'min_child_weight':1, 'gamma':1, \n",
    "# #               'max_delta_weight':11, 'random_state' :random}\n",
    "\n",
    "# params = lgbm_params\n",
    "# params['min_data_in_bin']=1\n",
    "# params['min_data']=1\n",
    "    \n",
    "# for set_val in demo_config[6] :\n",
    "#     curr_x_train, curr_y_train = pd.DataFrame(), []\n",
    "#     for k in all_curr_x_train_dict :\n",
    "#         if k != set_val :\n",
    "#             curr_x_train = curr_x_train.append(all_curr_x_train_dict[k])\n",
    "#             curr_y_train += all_curr_y_train_dict[k]\n",
    "\n",
    "#     t = get_target(model_name, curr_x_train, curr_y_train, curr_x_test, curr_y_test)\n",
    "#     xgbBO = BayesianOptimization(t.evaluate, \n",
    "#                                  get_params_range(model_name),\n",
    "#                                  random_state = random, \n",
    "#                                  verbose=0)\n",
    "\n",
    "#     xgbBO.maximize(init_points=5, n_iter=3)\n",
    "#     print(xgbBO.res['max']['max_params'])\n",
    "#     params = t.clean_param(xgbBO.res['max']['max_params'])\n",
    "\n",
    "#     model = reset_model(model_name, params)\n",
    "#     model.fit(curr_x_train, curr_y_train)\n",
    "#     pickle.dump(model, open(\"db/%s_%s_bayesian_transfer_%s.pickle.dat\" % ('PCI', model_name, set_val), \"wb\"))\n",
    "    \n",
    "#     y_pred = model.predict(all_curr_x_test_dict[set_val])\n",
    "#     predictions = [round(value) for value in y_pred]\n",
    "#     mse = metric.mean_squared_error(all_curr_y_test_dict[set_val], predictions)\n",
    "#     rmse = math.sqrt(mse)\n",
    "#     print(rmse)\n",
    "#     acc_dict[set_val] = [len(curr_x_train), len(curr_x_test), rmse]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
